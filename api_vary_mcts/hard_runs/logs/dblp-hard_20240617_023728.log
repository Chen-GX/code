toolqa_hard
dblp-hard
1
2024/06/17 02:38:20 log_utils.py[line:35] INFO seed:1718563100
2024/06/17 02:38:20 log_utils.py[line:35] INFO verbose:False
2024/06/17 02:38:20 log_utils.py[line:35] INFO process_num:1
2024/06/17 02:38:20 log_utils.py[line:35] INFO path:/mnt/workspace/nas/chenguoxin.cgx/api/datasets/ToolQA
2024/06/17 02:38:20 log_utils.py[line:35] INFO num_epoch:1
2024/06/17 02:38:20 log_utils.py[line:35] INFO tool_url:http://127.0.0.1:5010/toolqa
2024/06/17 02:38:20 log_utils.py[line:35] INFO filter:False
2024/06/17 02:38:20 log_utils.py[line:35] INFO filter_path:11
2024/06/17 02:38:20 log_utils.py[line:35] INFO checkpoint_dir:/mnt/workspace/nas/chenguoxin.cgx/model_cache/Meta-Llama-3-8B-Instruct
2024/06/17 02:38:20 log_utils.py[line:35] INFO temperature:0.5
2024/06/17 02:38:20 log_utils.py[line:35] INFO top_k:-1
2024/06/17 02:38:20 log_utils.py[line:35] INFO top_p:1.0
2024/06/17 02:38:20 log_utils.py[line:35] INFO frequency_penalty:1.2
2024/06/17 02:38:20 log_utils.py[line:35] INFO scratchpad_length:1024
2024/06/17 02:38:20 log_utils.py[line:35] INFO api_kernel_version:1
2024/06/17 02:38:20 log_utils.py[line:35] INFO Cpuct:1.25
2024/06/17 02:38:20 log_utils.py[line:35] INFO n_generate_sample:5
2024/06/17 02:38:20 log_utils.py[line:35] INFO max_iter:12
2024/06/17 02:38:20 log_utils.py[line:35] INFO max_depth:12
2024/06/17 02:38:20 log_utils.py[line:35] INFO positive_reward:1.0
2024/06/17 02:38:20 log_utils.py[line:35] INFO negative_reward:-1.0
2024/06/17 02:38:20 log_utils.py[line:35] INFO max_new_tokens:1024
2024/06/17 02:38:20 log_utils.py[line:35] INFO max_load_db:5
2024/06/17 02:38:20 log_utils.py[line:35] INFO debug_num:-1
2024/06/17 02:38:20 log_utils.py[line:35] INFO datapath:/mnt/workspace/nas/chenguoxin.cgx/api/workspace/data
2024/06/17 02:38:20 log_utils.py[line:35] INFO task:toolqa_hard
2024/06/17 02:38:20 log_utils.py[line:35] INFO dataname:dblp-hard
2024/06/17 02:38:20 log_utils.py[line:35] INFO output_dir:/mnt/workspace/nas/chenguoxin.cgx/api/workspace/output_dir/mcts/round1/run/toolqa_hard/dblp-hard/Meta-Llama-3-8B-Instruct/20240617_023820
2024/06/17 02:38:20 log_utils.py[line:35] INFO num_examples:2
2024/06/17 02:38:20 log_utils.py[line:35] INFO model_name:Meta-Llama-3-8B-Instruct
2024/06/17 02:38:20 log_utils.py[line:35] INFO timestamp:20240617_023820
2024/06/17 02:38:21 batch_search_generate.py[line:279] INFO ********** EPOCH 0 ***********
Execute:   0%|          | 0/963 [00:00<?, ?it/s]INFO 06-17 02:38:21 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='/mnt/workspace/nas/chenguoxin.cgx/model_cache/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='/mnt/workspace/nas/chenguoxin.cgx/model_cache/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=1718563100, served_model_name=/mnt/workspace/nas/chenguoxin.cgx/model_cache/Meta-Llama-3-8B-Instruct)
2024/06/17 02:38:21 batch_search_generate.py[line:93] ERROR llm error No module named 'modelscope'
Traceback (most recent call last):
  File "/mnt/workspace/nas/chenguoxin.cgx/api/workspace/api_vary_mcts/hard_runs/../src/batch_search_generate.py", line 44, in llm_generate
    llm = LLM(model=args.checkpoint_dir, tensor_parallel_size=1, seed=random_seed, swap_space=8)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/vary/lib/python3.11/site-packages/vllm/entrypoints/llm.py", line 144, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/vary/lib/python3.11/site-packages/vllm/engine/llm_engine.py", line 359, in from_engine_args
    engine = cls(
             ^^^^
  File "/opt/conda/envs/vary/lib/python3.11/site-packages/vllm/engine/llm_engine.py", line 212, in __init__
    self.tokenizer = self._init_tokenizer()
                     ^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/vary/lib/python3.11/site-packages/vllm/engine/llm_engine.py", line 408, in _init_tokenizer
    return get_tokenizer_group(self.parallel_config.tokenizer_pool_config,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/vary/lib/python3.11/site-packages/vllm/transformers_utils/tokenizer_group/__init__.py", line 20, in get_tokenizer_group
    return TokenizerGroup(**init_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/vary/lib/python3.11/site-packages/vllm/transformers_utils/tokenizer_group/tokenizer_group.py", line 23, in __init__
    self.tokenizer = get_tokenizer(self.tokenizer_id, **tokenizer_config)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/vary/lib/python3.11/site-packages/vllm/transformers_utils/tokenizer.py", line 72, in get_tokenizer
    from modelscope.hub.snapshot_download import snapshot_download
ModuleNotFoundError: No module named 'modelscope'
Execute:   0%|          | 0/963 [00:15<?, ?it/s]Execute:   0%|          | 0/963 [00:30<?, ?it/s]Execute:   0%|          | 0/963 [00:45<?, ?it/s]Execute:   0%|          | 0/963 [01:00<?, ?it/s]Execute:   0%|          | 0/963 [01:15<?, ?it/s]Execute:   0%|          | 0/963 [01:30<?, ?it/s]Execute:   0%|          | 0/963 [01:45<?, ?it/s]已终止
